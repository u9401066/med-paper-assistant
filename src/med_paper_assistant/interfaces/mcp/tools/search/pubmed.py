"""
PubMed Search Tools

search_literature, find_related_articles, find_citing_articles, search strategy
"""

import json
from mcp.server.fastmcp import FastMCP

from pubmed_search import PubMedClient
from med_paper_assistant.infrastructure.services import StrategyManager
from med_paper_assistant.infrastructure.logging import setup_logger

logger = setup_logger()


def format_search_results(results: list, include_doi: bool = True) -> str:
    """Format search results for display."""
    if not results:
        return "No results found."
        
    if "error" in results[0]:
        return f"Error searching PubMed: {results[0]['error']}"
        
    formatted_output = f"Found {len(results)} results:\n\n"
    for i, paper in enumerate(results, 1):
        formatted_output += f"{i}. **{paper['title']}**\n"
        authors = paper.get('authors', [])
        formatted_output += f"   Authors: {', '.join(authors[:3])}{' et al.' if len(authors) > 3 else ''}\n"
        journal = paper.get('journal', 'Unknown Journal')
        year = paper.get('year', '')
        volume = paper.get('volume', '')
        pages = paper.get('pages', '')
        
        journal_info = f"{journal} ({year})"
        if volume:
            journal_info += f"; {volume}"
            if pages:
                journal_info += f": {pages}"
        formatted_output += f"   Journal: {journal_info}\n"
        formatted_output += f"   PMID: {paper.get('pmid', '')}"
        
        if include_doi and paper.get('doi'):
            formatted_output += f" | DOI: {paper['doi']}"
        if paper.get('pmc_id'):
            formatted_output += f" | PMC: {paper['pmc_id']} ğŸ“„"
        
        formatted_output += "\n"
        
        abstract = paper.get('abstract', '')
        if abstract:
            formatted_output += f"   Abstract: {abstract[:200]}...\n"
        formatted_output += "\n"
        
    return formatted_output


def register_pubmed_tools(mcp: FastMCP, searcher: PubMedClient, strategy_manager: StrategyManager):
    """Register PubMed search tools."""
    
    @mcp.tool()
    def configure_search_strategy(criteria_json: str) -> str:
        """
        Save a structured search strategy.
        
        Args:
            criteria_json: JSON string with keys: keywords (list), exclusions (list), 
                          article_types (list), min_sample_size (int), date_range (str).
        """
        try:
            criteria = json.loads(criteria_json)
            return strategy_manager.save_strategy(criteria)
        except Exception as e:
            return f"Error configuring strategy: {str(e)}"

    @mcp.tool()
    def get_search_strategy() -> str:
        """Get the currently saved search strategy."""
        strategy = strategy_manager.load_strategy()
        if not strategy:
            return "No strategy saved."
        return json.dumps(strategy.dict(), indent=2)

    @mcp.tool()
    def search_literature(
        query: str = "", 
        limit: int = 5, 
        min_year: int = None, 
        max_year: int = None,
        date_from: str = None,
        date_to: str = None,
        date_type: str = "edat",
        article_type: str = None, 
        strategy: str = "relevance", 
        use_saved_strategy: bool = False
    ) -> str:
        """
        Search for medical literature based on a query using PubMed.
        
        Args:
            query: The search query (e.g., "diabetes treatment guidelines"). 
                   Required if use_saved_strategy is False.
            limit: The maximum number of results to return.
            min_year: Optional minimum publication year (e.g., 2020).
            max_year: Optional maximum publication year.
            date_from: Precise start date in YYYY/MM/DD format (e.g., "2025/10/01").
                       More precise than min_year. If provided, overrides min_year.
            date_to: Precise end date in YYYY/MM/DD format (e.g., "2025/11/28").
                     More precise than max_year. If provided, overrides max_year.
            date_type: Which date field to search. Options:
                       - "edat" (default): Entrez date - when added to PubMed (best for NEW articles)
                       - "pdat": Publication date
                       - "mdat": Modification date
            article_type: Optional article type (e.g., "Review", "Clinical Trial", "Meta-Analysis").
            strategy: Search strategy ("recent", "most_cited", "relevance", "impact", "agent_decided"). 
                     Default is "relevance".
            use_saved_strategy: If True, uses the criteria from configure_search_strategy.
        """
        logger.info(f"Searching literature: query='{query}', limit={limit}, strategy='{strategy}'")
        try:
            min_sample_size = None
            
            if use_saved_strategy:
                saved_criteria = strategy_manager.load_strategy()
                if saved_criteria:
                    query = strategy_manager.build_pubmed_query(saved_criteria)
                    min_sample_size = saved_criteria.min_sample_size
                    logger.info(f"Using saved strategy. Generated query: {query}")
                else:
                    return "Error: No saved strategy found. Please use configure_search_strategy first."
            
            if not query:
                return "Error: Query is required unless use_saved_strategy is True and a strategy is saved."

            results = searcher.search(
                query, limit, min_year, max_year, 
                article_type, strategy,
                date_from=date_from, date_to=date_to, date_type=date_type
            )
            
            if min_sample_size:
                results = searcher.filter_results(results, min_sample_size)
                
            return format_search_results(results[:limit])
        except Exception as e:
            logger.error(f"Search failed: {e}")
            return f"Error: {e}"

    @mcp.tool()
    def find_related_articles(pmid: str, limit: int = 5) -> str:
        """
        Find articles related to a given PubMed article.
        Uses PubMed's "Related Articles" feature to find similar papers.
        
        Args:
            pmid: PubMed ID of the source article.
            limit: Maximum number of related articles to return.
            
        Returns:
            List of related articles with details.
        """
        logger.info(f"Finding related articles for PMID: {pmid}")
        try:
            results = searcher.get_related_articles(pmid, limit)
            
            if not results:
                return f"No related articles found for PMID {pmid}."
            
            if "error" in results[0]:
                return f"Error finding related articles: {results[0]['error']}"
            
            output = f"ğŸ“š **Related Articles for PMID {pmid}** ({len(results)} found)\n\n"
            output += format_search_results(results)
            return output
        except Exception as e:
            logger.error(f"Find related articles failed: {e}")
            return f"Error: {e}"

    @mcp.tool()
    def find_citing_articles(pmid: str, limit: int = 10) -> str:
        """
        Find articles that cite a given PubMed article.
        Uses PubMed Central's citation data to find papers that reference this article.
        
        Args:
            pmid: PubMed ID of the source article.
            limit: Maximum number of citing articles to return.
            
        Returns:
            List of citing articles with details.
        """
        logger.info(f"Finding citing articles for PMID: {pmid}")
        try:
            results = searcher.get_citing_articles(pmid, limit)
            
            if not results:
                return f"No citing articles found for PMID {pmid}. (Article may not be indexed in PMC or has no citations yet.)"
            
            if "error" in results[0]:
                return f"Error finding citing articles: {results[0]['error']}"
            
            output = f"ğŸ“– **Articles Citing PMID {pmid}** ({len(results)} found)\n\n"
            output += format_search_results(results)
            return output
        except Exception as e:
            logger.error(f"Find citing articles failed: {e}")
            return f"Error: {e}"

    @mcp.tool()
    def generate_search_queries(
        topic: str,
        strategy: str = "comprehensive",
        include_mesh: bool = True,
        include_synonyms: bool = True,
        use_saved_strategy: bool = True
    ) -> str:
        """
        æ ¹æ“šä¸»é¡Œç”Ÿæˆå¤šçµ„æœå°‹èªæ³•ï¼Œä¾›ä¸¦è¡Œæœå°‹ä½¿ç”¨ã€‚
        
        é€™å€‹å·¥å…·è¿”å›å¤šå€‹æœå°‹ queriesï¼ŒAgent æ‡‰è©²**ä¸¦è¡Œå‘¼å«** search_literature
        å°æ¯å€‹ query åŸ·è¡Œæœå°‹ï¼Œç„¶å¾Œä½¿ç”¨ merge_search_results åˆä½µçµæœã€‚
        
        Args:
            topic: æœå°‹ä¸»é¡Œï¼ˆå¦‚ "remimazolam ICU sedation"ï¼‰
            strategy: æœå°‹ç­–ç•¥
                - "comprehensive": å…¨é¢æœå°‹ï¼Œå¤šçµ„ä¸åŒè§’åº¦çš„ queries
                - "focused": ç²¾ç¢ºæœå°‹ï¼Œè¼ƒå°‘ä½†æ›´ç²¾ç¢ºçš„ queries  
                - "exploratory": æ¢ç´¢æ€§æœå°‹ï¼ŒåŒ…å«æ›´å»£æ³›çš„ç›¸é—œæ¦‚å¿µ
            include_mesh: æ˜¯å¦åŒ…å« MeSH è©å½™çš„æœå°‹
            include_synonyms: æ˜¯å¦åŒ…å«åŒç¾©è©/åˆ¥å
            use_saved_strategy: æ˜¯å¦ä½¿ç”¨å·²å„²å­˜çš„æœå°‹ç­–ç•¥ï¼ˆdate_range, exclusions, article_typesï¼‰
            
        Returns:
            JSON æ ¼å¼çš„æœå°‹ç­–ç•¥ï¼ŒåŒ…å«å¤šå€‹ queries ä¾›ä¸¦è¡ŒåŸ·è¡Œ
        """
        logger.info(f"Generating search queries for topic: {topic}, strategy: {strategy}")
        
        # è¼‰å…¥å·²å„²å­˜çš„ç­–ç•¥è¨­å®š
        saved_strategy = None
        date_filter = ""
        exclusion_filter = ""
        article_type_filter = ""
        
        if use_saved_strategy:
            saved_strategy = strategy_manager.load_strategy()
            if saved_strategy:
                logger.info(f"Using saved strategy: {saved_strategy}")
                
                # è™•ç†æ—¥æœŸç¯„åœ
                if saved_strategy.date_range:
                    # æ”¯æ´æ ¼å¼: "2020-2024" æˆ– "5 years" æˆ– "last 10 years"
                    dr = saved_strategy.date_range.lower()
                    if "-" in dr and len(dr.split("-")) == 2:
                        parts = dr.split("-")
                        if parts[0].isdigit() and parts[1].isdigit():
                            date_filter = f" AND ({parts[0]}:{parts[1]}[dp])"
                    elif "year" in dr:
                        import re
                        match = re.search(r'(\d+)\s*year', dr)
                        if match:
                            years = int(match.group(1))
                            from datetime import datetime
                            current_year = datetime.now().year
                            start_year = current_year - years
                            date_filter = f" AND ({start_year}:{current_year}[dp])"
                
                # è™•ç†æ’é™¤è©
                if saved_strategy.exclusions:
                    exclusions = [f'NOT "{ex}"' for ex in saved_strategy.exclusions]
                    exclusion_filter = " " + " ".join(exclusions)
                
                # è™•ç†æ–‡ç« é¡å‹
                if saved_strategy.article_types:
                    types = [f'"{t}"[Publication Type]' for t in saved_strategy.article_types]
                    article_type_filter = f" AND ({' OR '.join(types)})"
        
        # è§£æä¸»é¡Œè©å½™
        words = topic.lower().split()
        
        # å»ºç«‹éæ¿¾å™¨å­—ä¸²ï¼ˆç”¨æ–¼è¿½åŠ åˆ°æ¯å€‹æŸ¥è©¢ï¼‰
        filters = f"{date_filter}{article_type_filter}{exclusion_filter}".strip()
        
        queries = []
        
        # Query 1: ç²¾ç¢ºæ¨™é¡Œæœå°‹
        base_q1 = f"({topic})[Title]"
        queries.append({
            "id": "q1_title",
            "query": f"{base_q1}{filters}" if filters else base_q1,
            "purpose": "ç²¾ç¢ºæ¨™é¡ŒåŒ¹é…",
            "expected": "é«˜ç›¸é—œæ€§ï¼Œè¼ƒå°‘çµæœ"
        })
        
        # Query 2: æ¨™é¡Œ/æ‘˜è¦æœå°‹
        base_q2 = f"({topic})[Title/Abstract]"
        queries.append({
            "id": "q2_tiab",
            "query": f"{base_q2}{filters}" if filters else base_q2,
            "purpose": "æ¨™é¡Œæˆ–æ‘˜è¦åŒ…å«é—œéµå­—",
            "expected": "ä¸­ç­‰ç›¸é—œæ€§ï¼Œé©é‡çµæœ"
        })
        
        # Query 3: çµ„åˆè©æœå°‹ï¼ˆç”¨ AND é€£æ¥ï¼‰
        and_query = " AND ".join(words)
        base_q3 = f"({and_query})"
        queries.append({
            "id": "q3_and",
            "query": f"{base_q3}{filters}" if filters else and_query,
            "purpose": "æ‰€æœ‰é—œéµå­—éƒ½å¿…é ˆå‡ºç¾",
            "expected": "è¼ƒåš´æ ¼çš„ç¯©é¸"
        })
        
        if strategy in ["comprehensive", "exploratory"]:
            # Query 4: éƒ¨åˆ†è©å½™æœå°‹ï¼ˆæ“´å±•ï¼‰
            if len(words) >= 2:
                # å–ä¸»è¦è©å½™çµ„åˆ
                main_word = words[0]
                context_words = " OR ".join(words[1:])
                base_q4 = f"({main_word} AND ({context_words}))"
                queries.append({
                    "id": "q4_partial",
                    "query": f"{base_q4}{filters}" if filters else f"{main_word} AND ({context_words})",
                    "purpose": "ä¸»è¦è©å½™ + ä»»ä¸€æƒ…å¢ƒè©",
                    "expected": "è¼ƒå¯¬é¬†çš„åŒ¹é…"
                })
        
        if include_mesh:
            # Query 5: MeSH è©å½™æœå°‹
            base_q5 = f"({topic})[MeSH Terms]"
            queries.append({
                "id": "q5_mesh",
                "query": f"{base_q5}{filters}" if filters else base_q5,
                "purpose": "ä½¿ç”¨ MeSH æ¨™æº–è©å½™",
                "expected": "é†«å­¸æ¦‚å¿µæ¨™æº–åŒ–åŒ¹é…"
            })
        
        if strategy == "exploratory":
            # Query 6: ç›¸é—œæ¦‚å¿µæ“´å±•
            # é€™è£¡å¯ä»¥åŠ å…¥æ›´å¤šé ˜åŸŸçŸ¥è­˜
            base_q6 = f"({words[0]})[Title] AND review[Publication Type]"
            queries.append({
                "id": "q6_broad",
                "query": f"{base_q6}{date_filter}{exclusion_filter}" if (date_filter or exclusion_filter) else base_q6,
                "purpose": "æ‰¾ç›¸é—œçš„ Review æ–‡ç« ",
                "expected": "äº†è§£é ˜åŸŸå…¨è²Œ"
            })
        
        # æ§‹å»ºçµæœ
        result = {
            "topic": topic,
            "strategy": strategy,
            "queries_count": len(queries),
            "queries": queries,
            "instruction": "è«‹ä¸¦è¡Œå‘¼å« search_literature å°æ¯å€‹ query åŸ·è¡Œæœå°‹ï¼Œ" +
                          "ç„¶å¾Œå‘¼å« merge_search_results åˆä½µçµæœã€‚",
            "example": {
                "parallel_calls": [
                    f"search_literature(query=\"{q['query']}\", limit=20)" 
                    for q in queries[:2]
                ] + ["..."]
            }
        }
        
        # åŠ å…¥å·²æ‡‰ç”¨çš„ç­–ç•¥è³‡è¨Š
        if saved_strategy:
            result["applied_strategy"] = {
                "date_range": saved_strategy.date_range or "not set",
                "exclusions": saved_strategy.exclusions or [],
                "article_types": saved_strategy.article_types or [],
                "note": "å·²å„²å­˜çš„æœå°‹ç­–ç•¥å·²è‡ªå‹•æ•´åˆåˆ°æŸ¥è©¢ä¸­"
            }
        else:
            result["applied_strategy"] = None
            result["tip"] = "å¯ä½¿ç”¨ configure_search_strategy è¨­å®šæ—¥æœŸç¯„åœã€æ’é™¤è©ç­‰ï¼Œä¸‹æ¬¡ç”ŸæˆæŸ¥è©¢æ™‚æœƒè‡ªå‹•å¥—ç”¨"
        
        return json.dumps(result, indent=2, ensure_ascii=False)

    @mcp.tool()
    def merge_search_results(results_json: str) -> str:
        """
        åˆä½µå¤šå€‹æœå°‹çµæœä¸¦å»é‡ã€‚
        
        åœ¨ä¸¦è¡ŒåŸ·è¡Œå¤šå€‹ search_literature å¾Œï¼Œä½¿ç”¨æ­¤å·¥å…·åˆä½µçµæœã€‚
        
        Args:
            results_json: JSON æ ¼å¼çš„æœå°‹çµæœé™£åˆ—ï¼Œæ¯å€‹å…ƒç´ åŒ…å«ï¼š
                - query_id: æœå°‹ IDï¼ˆå°æ‡‰ generate_search_queries è¿”å›çš„ idï¼‰
                - pmids: PMID åˆ—è¡¨
                
                ä¾‹å¦‚ï¼š
                [
                    {"query_id": "q1_title", "pmids": ["12345", "67890"]},
                    {"query_id": "q2_tiab", "pmids": ["67890", "11111"]}
                ]
                
        Returns:
            åˆä½µå¾Œçš„çµæœï¼ŒåŒ…å«å»é‡å¾Œçš„ PMID åˆ—è¡¨å’Œä¾†æºåˆ†æ
        """
        logger.info("Merging search results")
        
        try:
            results = json.loads(results_json)
        except json.JSONDecodeError as e:
            return f"Error: Invalid JSON format - {e}"
        
        # æ”¶é›†æ‰€æœ‰ PMID å’Œä¾†æº
        pmid_sources = {}  # pmid -> [source_ids]
        all_pmids = []
        
        for result in results:
            query_id = result.get("query_id", "unknown")
            pmids = result.get("pmids", [])
            
            for pmid in pmids:
                pmid = str(pmid).strip()
                if pmid not in pmid_sources:
                    pmid_sources[pmid] = []
                    all_pmids.append(pmid)
                pmid_sources[pmid].append(query_id)
        
        # åˆ†æçµæœ
        multi_source = {pmid: sources for pmid, sources in pmid_sources.items() if len(sources) > 1}
        single_source = {pmid: sources[0] for pmid, sources in pmid_sources.items() if len(sources) == 1}
        
        # æŒ‰ä¾†æºåˆ†çµ„
        by_query = {}
        for result in results:
            query_id = result.get("query_id", "unknown")
            by_query[query_id] = len(result.get("pmids", []))
        
        output = {
            "total_unique": len(all_pmids),
            "total_with_duplicates": sum(by_query.values()),
            "duplicates_removed": sum(by_query.values()) - len(all_pmids),
            "by_query": by_query,
            "appeared_in_multiple_queries": {
                "count": len(multi_source),
                "pmids": list(multi_source.keys())[:10],  # åªé¡¯ç¤ºå‰ 10 å€‹
                "note": "é€™äº›æ–‡ç»è¢«å¤šå€‹æœå°‹ç­–ç•¥æ‰¾åˆ°ï¼Œå¯èƒ½æ›´ç›¸é—œ"
            },
            "unique_pmids": all_pmids,
            "next_step": "ä½¿ç”¨ save_reference(pmid=...) å„²å­˜æ„Ÿèˆˆè¶£çš„æ–‡ç»ï¼Œ" +
                        "æˆ–ä½¿ç”¨ get_reference_details(pmid=...) å–å¾—è©³ç´°è³‡è¨Š",
            "need_more": "å¦‚æœçµæœä¸å¤ ï¼Œä½¿ç”¨ expand_search_queries ç”Ÿæˆæ›´å¤šæœå°‹ç­–ç•¥"
        }
        
        return json.dumps(output, indent=2, ensure_ascii=False)

    @mcp.tool()
    def expand_search_queries(
        topic: str,
        existing_query_ids: str = "",
        expansion_type: str = "synonyms",
        use_saved_strategy: bool = True
    ) -> str:
        """
        æ“´å±•æœå°‹æŸ¥è©¢ï¼Œç•¶åˆå§‹æœå°‹çµæœä¸å¤ æ™‚ä½¿ç”¨ã€‚
        
        é€™å€‹å·¥å…·ç”Ÿæˆ**é¡å¤–çš„**æœå°‹ç­–ç•¥ï¼Œèˆ‡åˆå§‹æŸ¥è©¢ä¸é‡è¤‡ã€‚
        é©åˆåœ¨ generate_search_queries + merge_search_results å¾Œï¼Œ
        ç™¼ç¾çµæœä¸å¤ æ™‚ä½¿ç”¨ã€‚
        
        Args:
            topic: åŸå§‹æœå°‹ä¸»é¡Œ
            existing_query_ids: å·²åŸ·è¡Œçš„æŸ¥è©¢ IDï¼ˆé€—è™Ÿåˆ†éš”ï¼‰ï¼Œé¿å…é‡è¤‡
                              ä¾‹å¦‚ï¼š"q1_title,q2_tiab,q3_and"
            expansion_type: æ“´å±•é¡å‹
                - "synonyms": åŒç¾©è©æ“´å±•ï¼ˆå¦‚ sedation â†’ conscious sedation, procedural sedationï¼‰
                - "related": ç›¸é—œæ¦‚å¿µï¼ˆå¦‚ ICU â†’ critical care, intensive careï¼‰
                - "broader": æ›´å»£æ³›çš„æœå°‹ï¼ˆæ”¾å¯¬é™åˆ¶ï¼‰
                - "narrower": æ›´ç²¾ç¢ºçš„æœå°‹ï¼ˆåŠ å¼·é™åˆ¶ï¼‰
                - "author": æœå°‹é—œéµä½œè€…çš„å…¶ä»–æ–‡ç»
                - "citation": åŸºæ–¼å·²æ‰¾åˆ°æ–‡ç»çš„å¼•ç”¨ç¶²çµ¡
            use_saved_strategy: æ˜¯å¦å¥—ç”¨å·²å„²å­˜çš„æœå°‹ç­–ç•¥
            
        Returns:
            æ–°çš„æœå°‹æŸ¥è©¢ï¼Œå¯ä¸¦è¡ŒåŸ·è¡Œå¾Œèˆ‡ä¹‹å‰çµæœåˆä½µ
        """
        logger.info(f"Expanding search for topic: {topic}, type: {expansion_type}")
        
        # è§£æå·²åŸ·è¡Œçš„æŸ¥è©¢
        existing = set(existing_query_ids.split(",")) if existing_query_ids else set()
        
        # è¼‰å…¥ç­–ç•¥è¨­å®š
        filters = ""
        if use_saved_strategy:
            saved_strategy = strategy_manager.load_strategy()
            if saved_strategy:
                date_filter = ""
                exclusion_filter = ""
                article_type_filter = ""
                
                if saved_strategy.date_range:
                    dr = saved_strategy.date_range.lower()
                    if "-" in dr and len(dr.split("-")) == 2:
                        parts = dr.split("-")
                        if parts[0].isdigit() and parts[1].isdigit():
                            date_filter = f" AND ({parts[0]}:{parts[1]}[dp])"
                
                if saved_strategy.exclusions:
                    exclusions = [f'NOT "{ex}"' for ex in saved_strategy.exclusions]
                    exclusion_filter = " " + " ".join(exclusions)
                
                if saved_strategy.article_types:
                    types = [f'"{t}"[Publication Type]' for t in saved_strategy.article_types]
                    article_type_filter = f" AND ({' OR '.join(types)})"
                
                filters = f"{date_filter}{article_type_filter}{exclusion_filter}".strip()
        
        words = topic.lower().split()
        queries = []
        query_counter = len(existing) + 1
        
        if expansion_type == "synonyms":
            # ç”ŸæˆåŒç¾©è©æ“´å±•æŸ¥è©¢
            # å¸¸è¦‹é†«å­¸åŒç¾©è©å°ç…§
            synonym_map = {
                "sedation": ["conscious sedation", "procedural sedation", "moderate sedation", "deep sedation"],
                "icu": ["intensive care unit", "critical care unit", "CCU"],
                "anesthesia": ["anaesthesia", "anesthetic", "anaesthetic"],
                "pain": ["analgesia", "analgesic", "nociception"],
                "surgery": ["surgical", "operative", "perioperative", "intraoperative"],
                "ventilation": ["mechanical ventilation", "respiratory support", "ventilator"],
                "hypotension": ["low blood pressure", "hemodynamic instability"],
                "mortality": ["death", "survival", "fatality"],
                "prediction": ["predictive", "prognostic", "forecasting"],
                "machine learning": ["ML", "artificial intelligence", "AI", "deep learning", "neural network"],
            }
            
            for word in words:
                word_lower = word.lower()
                if word_lower in synonym_map:
                    for synonym in synonym_map[word_lower][:2]:  # å–å‰ 2 å€‹åŒç¾©è©
                        new_topic = topic.replace(word, synonym)
                        query_id = f"q{query_counter}_syn_{word_lower[:3]}"
                        if query_id not in existing:
                            base_q = f"({new_topic})[Title/Abstract]"
                            queries.append({
                                "id": query_id,
                                "query": f"{base_q}{filters}" if filters else base_q,
                                "purpose": f"åŒç¾©è©æ“´å±•: {word} â†’ {synonym}",
                                "expected": "æ‰¾åˆ°ä½¿ç”¨ä¸åŒè¡“èªçš„ç›¸é—œæ–‡ç»"
                            })
                            query_counter += 1
                            
        elif expansion_type == "related":
            # ç›¸é—œæ¦‚å¿µæ“´å±•
            related_concepts = {
                "sedation": ["analgesia", "anxiolysis", "hypnotic"],
                "icu": ["emergency department", "operating room", "PACU", "ward"],
                "anesthesia": ["sedation", "regional block", "nerve block"],
                "remimazolam": ["midazolam", "propofol", "dexmedetomidine", "benzodiazepine"],
                "propofol": ["remimazolam", "etomidate", "ketamine"],
                "hypotension": ["bradycardia", "tachycardia", "arrhythmia", "shock"],
            }
            
            for word in words:
                word_lower = word.lower()
                if word_lower in related_concepts:
                    for related in related_concepts[word_lower][:2]:
                        other_words = [w for w in words if w.lower() != word_lower]
                        if other_words:
                            new_topic = f"{related} {' '.join(other_words)}"
                            query_id = f"q{query_counter}_rel_{word_lower[:3]}"
                            if query_id not in existing:
                                base_q = f"({new_topic})[Title/Abstract]"
                                queries.append({
                                    "id": query_id,
                                    "query": f"{base_q}{filters}" if filters else base_q,
                                    "purpose": f"ç›¸é—œæ¦‚å¿µ: {word} â†’ {related}",
                                    "expected": "æ‰¾åˆ°ç›¸é—œä½†ä¸åŒä¸»é¡Œçš„æ–‡ç»"
                                })
                                query_counter += 1
                                
        elif expansion_type == "broader":
            # æ›´å»£æ³›çš„æœå°‹ï¼ˆæ”¾å¯¬é™åˆ¶ï¼‰
            # ä½¿ç”¨ OR è€Œé AND
            if len(words) >= 2:
                or_query = " OR ".join(words)
                query_id = f"q{query_counter}_broad_or"
                if query_id not in existing:
                    queries.append({
                        "id": query_id,
                        "query": f"({or_query})[Title/Abstract]{filters}",
                        "purpose": "æ”¾å¯¬æœå°‹ï¼šä»»ä¸€é—œéµå­—",
                        "expected": "æ›´å¤šçµæœï¼Œç›¸é—œæ€§å¯èƒ½è¼ƒä½"
                    })
                    query_counter += 1
            
            # åªç”¨ä¸»è¦è©å½™
            main_word = words[0]
            query_id = f"q{query_counter}_broad_main"
            if query_id not in existing:
                queries.append({
                    "id": query_id,
                    "query": f"({main_word})[Title]{filters}",
                    "purpose": f"åªæœå°‹ä¸»è¦è©å½™: {main_word}",
                    "expected": "æ›´å»£æ³›çš„çµæœ"
                })
                query_counter += 1
                
            # ç§»é™¤æ—¥æœŸé™åˆ¶ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
            if filters and "[dp]" in filters:
                no_date_filters = filters.split(" AND ")[0]  # ç§»é™¤æ—¥æœŸéƒ¨åˆ†
                query_id = f"q{query_counter}_broad_nodate"
                if query_id not in existing:
                    queries.append({
                        "id": query_id,
                        "query": f"({topic})[Title/Abstract]",  # ç„¡æ—¥æœŸé™åˆ¶
                        "purpose": "ç§»é™¤æ—¥æœŸé™åˆ¶",
                        "expected": "åŒ…å«è¼ƒèˆŠçš„æ–‡ç»"
                    })
                    query_counter += 1
                    
        elif expansion_type == "narrower":
            # æ›´ç²¾ç¢ºçš„æœå°‹
            # åŠ å…¥æ›´å¤šé™åˆ¶
            query_id = f"q{query_counter}_narrow_rct"
            if query_id not in existing:
                queries.append({
                    "id": query_id,
                    "query": f"({topic})[Title] AND (randomized controlled trial[pt] OR RCT[tiab]){filters}",
                    "purpose": "é™å®š RCT ç ”ç©¶",
                    "expected": "é«˜å“è³ªè­‰æ“š"
                })
                query_counter += 1
            
            query_id = f"q{query_counter}_narrow_meta"
            if query_id not in existing:
                queries.append({
                    "id": query_id,
                    "query": f"({topic})[Title/Abstract] AND (meta-analysis[pt] OR systematic review[pt]){filters}",
                    "purpose": "é™å®š Meta-analysis/SR",
                    "expected": "ç¶œåˆæ€§è­‰æ“š"
                })
                query_counter += 1
                
            query_id = f"q{query_counter}_narrow_recent"
            if query_id not in existing:
                from datetime import datetime
                current_year = datetime.now().year
                queries.append({
                    "id": query_id,
                    "query": f"({topic})[Title] AND ({current_year-2}:{current_year}[dp])",
                    "purpose": "é™å®šæœ€è¿‘ 2 å¹´",
                    "expected": "æœ€æ–°ç ”ç©¶"
                })
                query_counter += 1
        
        # å¦‚æœæ²’æœ‰ç”Ÿæˆä»»ä½•æŸ¥è©¢ï¼Œæä¾›é è¨­æ“´å±•
        if not queries:
            # ä½¿ç”¨è¬ç”¨å¡æ“´å±•
            query_id = f"q{query_counter}_wildcard"
            queries.append({
                "id": query_id,
                "query": f"({words[0]}*)[Title/Abstract]{filters}",
                "purpose": "è¬ç”¨å¡æ“´å±•",
                "expected": "åŒ…å«è©å¹¹è®ŠåŒ–"
            })
            query_counter += 1
            
            # ä½¿ç”¨ All Fields
            query_id = f"q{query_counter}_allfields"
            queries.append({
                "id": query_id,
                "query": f"({topic})[All Fields]{filters}",
                "purpose": "æœå°‹æ‰€æœ‰æ¬„ä½",
                "expected": "æœ€å»£æ³›çš„æœå°‹"
            })
        
        result = {
            "topic": topic,
            "expansion_type": expansion_type,
            "existing_queries": list(existing),
            "new_queries_count": len(queries),
            "queries": queries,
            "instruction": "è«‹ä¸¦è¡ŒåŸ·è¡Œé€™äº›æ–°æŸ¥è©¢ï¼Œç„¶å¾Œå°‡çµæœèˆ‡ä¹‹å‰çš„çµæœä¸€èµ·å‚³çµ¦ merge_search_results",
            "available_expansion_types": [
                {"type": "synonyms", "description": "åŒç¾©è©æ“´å±•"},
                {"type": "related", "description": "ç›¸é—œæ¦‚å¿µ"},
                {"type": "broader", "description": "æ”¾å¯¬é™åˆ¶"},
                {"type": "narrower", "description": "æ›´ç²¾ç¢ºæœå°‹"},
            ]
        }
        
        return json.dumps(result, indent=2, ensure_ascii=False)
