"""
Spark-Soup: Context Stuffing for Creativity

ç”¨ç¢ç‰‡åŒ–è³‡è¨Šå¡«å…… contextï¼Œæ¨¡æ“¬äººé¡æ¥æ”¶æ–°è/æ›¸ç±/é«”é©—å¾Œç”¢ç”Ÿå‰µæ„é€£çµçš„éç¨‹ã€‚

æ ¸å¿ƒæ¦‚å¿µï¼š
1. æ”¶é›†å¤šä¾†æºç¢ç‰‡ï¼ˆæœå°‹ã€ç¶­åŸºã€åè¨€ç­‰ï¼‰
2. çµ„è£ã€Œå‰µæ„æ¹¯ã€ä¸¦é‡è¤‡éŒ¨å®šä¸»é¡Œ
3. è®“ LLM å¾ç¢ç‰‡ä¸­ç”¢ç”Ÿæ„å¤–é€£çµ
"""

import logging
import random
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)


class FragmentSource(Enum):
    """ç¢ç‰‡ä¾†æº"""

    DUCKDUCKGO = "duckduckgo"
    WIKIPEDIA = "wikipedia"
    QUOTES = "quotes"
    CONCEPTNET = "conceptnet"
    USER = "user"
    RANDOM = "random"


@dataclass
class Fragment:
    """å‰µæ„ç¢ç‰‡"""

    content: str
    source: FragmentSource
    relevance: float = 0.5  # 0-1ï¼Œèˆ‡ä¸»é¡Œçš„ç›¸é—œæ€§

    def __str__(self) -> str:
        return f"ğŸ“Œ {self.content}"


@dataclass
class SparkSoupResult:
    """Spark Soup çµæœ"""

    soup: str
    topic: str
    fragments_used: list[Fragment]
    diversity_score: float
    trigger_words_used: list[str]

    def to_dict(self) -> dict:
        return {
            "soup": self.soup,
            "topic": self.topic,
            "fragments_count": len(self.fragments_used),
            "diversity_score": self.diversity_score,
            "trigger_words": self.trigger_words_used,
            "sources": list(set(f.source.value for f in self.fragments_used)),
        }


@dataclass
class Idea:
    """ç”Ÿæˆçš„æƒ³æ³•"""

    title: str
    description: str
    connected_fragments: list[str]
    novelty_score: float

    def to_dict(self) -> dict:
        return {
            "title": self.title,
            "description": self.description,
            "connected_fragments": self.connected_fragments,
            "novelty_score": self.novelty_score,
        }


# === å…§å»ºè³‡æ–™åº« ===

TRIGGER_WORDS = {
    "combination": [
        "å¦‚æœæŠŠé€™å…©å€‹çµåˆæœƒæ€æ¨£ï¼Ÿ",
        "é€™äº›æ¦‚å¿µæœ‰ä»€éº¼å…±é€šé»ï¼Ÿ",
        "æŠŠé€™å€‹æ”¾åˆ°å¦ä¸€å€‹é ˜åŸŸæœƒè®Šæˆä»€éº¼ï¼Ÿ",
        "å¦‚æœ A é‡åˆ° B æœƒç™¼ç”Ÿä»€éº¼ï¼Ÿ",
    ],
    "inversion": [
        "å¦‚æœå®Œå…¨ç›¸åæœƒæ€æ¨£ï¼Ÿ",
        "æŠŠé€™å€‹é¡›å€’éä¾†å‘¢ï¼Ÿ",
        "å¦‚æœç¼ºé»è®Šæˆå„ªé»å‘¢ï¼Ÿ",
        "å¦‚æœç¦æ­¢æœ€å¸¸è¦‹çš„åšæ³•å‘¢ï¼Ÿ",
    ],
    "scale": [
        "å¦‚æœæ”¾å¤§ 10 å€å‘¢ï¼Ÿ",
        "å¦‚æœç¸®å°åˆ°æ¥µè‡´å‘¢ï¼Ÿ",
        "å¦‚æœçµ¦ç„¡é™è³‡æºå‘¢ï¼Ÿ",
        "å¦‚æœåªæœ‰ 1% çš„è³‡æºå‘¢ï¼Ÿ",
    ],
    "time": [
        "100 å¹´å¾Œæœƒè®Šæˆä»€éº¼æ¨£å­ï¼Ÿ",
        "å¦‚æœåœ¨å¤ä»£å°±æœ‰é€™å€‹å‘¢ï¼Ÿ",
        "å¦‚æœå¿…é ˆåœ¨ 1 å°æ™‚å…§å®Œæˆå‘¢ï¼Ÿ",
        "å¦‚æœå¯ä»¥æ™‚é–“æ—…è¡Œå‘¢ï¼Ÿ",
    ],
    "perspective": [
        "å¦‚æœæ˜¯å°å­©ä¾†çœ‹é€™å€‹å•é¡Œå‘¢ï¼Ÿ",
        "å¦‚æœæ˜¯å¤–æ˜Ÿäººç¬¬ä¸€æ¬¡çœ‹åˆ°å‘¢ï¼Ÿ",
        "å¦‚æœç«¶çˆ­å°æ‰‹é€™æ¨£åšå‘¢ï¼Ÿ",
        "å¦‚æœæ˜¯å‹•ç‰©/æ¤ç‰©çš„è¦–è§’å‘¢ï¼Ÿ",
    ],
    "emotion": [
        "å¦‚æœè®“äººæ„Ÿåˆ°é©šå–œå‘¢ï¼Ÿ",
        "å¦‚æœè®“äººå¤§ç¬‘å‘¢ï¼Ÿ",
        "å¦‚æœè®“äººæ„Ÿå‹•è½æ·šå‘¢ï¼Ÿ",
        "å¦‚æœè®“äººæ„Ÿåˆ°èˆ’é©å‘¢ï¼Ÿ",
    ],
}

CREATIVITY_QUOTES = [
    "å‰µæ„å°±æ˜¯é€£çµäº‹ç‰©ã€‚â€” Steve Jobs",
    "é™åˆ¶æ¿€ç™¼å‰µæ„ã€‚",
    "å¥½çš„è—è¡“å®¶è¤‡è£½ï¼Œå‰å¤§çš„è—è¡“å®¶å·ç«Šã€‚â€” Picasso",
    "æ¯å€‹å­©å­éƒ½æ˜¯è—è¡“å®¶ï¼Œå•é¡Œæ˜¯é•·å¤§å¾Œå¦‚ä½•ä¿æŒé€™ç¨®èƒ½åŠ›ã€‚â€” Picasso",
    "æƒ³åƒåŠ›æ¯”çŸ¥è­˜æ›´é‡è¦ã€‚â€” Einstein",
    "å‰µæ„éœ€è¦å‹‡æ°£ã€‚â€” Henri Matisse",
    "æˆ‘æ²’æœ‰ç‰¹åˆ¥çš„å¤©è³¦ï¼Œåªæ˜¯å¼·çƒˆå¥½å¥‡å¿ƒã€‚â€” Einstein",
    "æœ€å¥½çš„é æ¸¬æœªä¾†çš„æ–¹å¼å°±æ˜¯å‰µé€ å®ƒã€‚â€” Peter Drucker",
    "çªç ´å¸¸è¦æ˜¯å‰µæ–°çš„é–‹å§‹ã€‚",
    "å•å°å•é¡Œæ¯”æ‰¾åˆ°ç­”æ¡ˆæ›´é‡è¦ã€‚",
    "å¤±æ•—æ˜¯æˆåŠŸä¹‹æ¯ï¼Œæ¯ä¸€æ¬¡å¤±æ•—éƒ½æ˜¯å­¸ç¿’ã€‚",
    "ç°¡å–®æ˜¯çµ‚æ¥µçš„è¤‡é›œã€‚â€” Da Vinci",
    "å‰µæ„æ˜¯ä¸€ç¨®ç¿’æ…£ï¼Œè€Œä¸æ˜¯å¶ç„¶ã€‚",
    "æ‰“ç ´è¦å‰‡ä¹‹å‰ï¼Œå…ˆäº†è§£è¦å‰‡ã€‚",
    "é‚Šç•Œå­˜åœ¨æ˜¯ç‚ºäº†è¢«è·¨è¶Šã€‚",
    "ä¸åŒçš„è§€é»å¸¶ä¾†ä¸åŒçš„è§£æ±ºæ–¹æ¡ˆã€‚",
    "å‰µæ„ä¾†è‡ªæ–¼ä¸åŒæƒ³æ³•çš„ç¢°æ’ã€‚",
    "ä¿æŒå¥½å¥‡å¿ƒï¼Œæ°¸é åƒå€‹å­©å­ã€‚",
    "å‰µæ„æ˜¯çœ‹è¦‹åˆ¥äººçœ‹ä¸è¦‹çš„é€£çµã€‚",
    "æœ€ç˜‹ç‹‚çš„æƒ³æ³•å¾€å¾€æœ€æœ‰åƒ¹å€¼ã€‚",
]

RANDOM_CONCEPTS = [
    "èœ‚å·¢",
    "æ˜Ÿç©º",
    "å’–å•¡",
    "æ£®æ—",
    "æ©Ÿå™¨äºº",
    "éŸ³æ¨‚",
    "æµ·æ´‹",
    "å¤¢æƒ³",
    "æ—…è¡Œ",
    "é­”æ³•",
    "æ™‚é–“",
    "å½©è™¹",
    "é¢¨ç®",
    "è¢ç«èŸ²",
    "ç©æœ¨",
    "æ‹¼åœ–",
    "è¿·å®®",
    "æœ›é é¡",
    "é¡¯å¾®é¡",
    "è Ÿç‡­",
    "é¡å­",
    "å½±å­",
    "æ²³æµ",
    "ç€‘å¸ƒ",
    "ç«å±±",
    "å†°å±±",
    "æ²™æ¼ ",
    "ç¶ æ´²",
    "ç‡ˆå¡”",
    "ç¾…ç›¤",
    "åœ°åœ–",
    "å¯¶è—",
    "ç¨®å­",
    "è´è¶",
    "èœ»èœ“",
    "çŠç‘š",
    "æ™¨éœ²",
    "é»ƒæ˜",
    "æ¥µå…‰",
    "é–ƒé›»",
]

CROSS_DOMAIN_CONCEPTS = {
    "ç§‘æŠ€": ["AI", "å€å¡Šéˆ", "é‡å­è¨ˆç®—", "ç‰©è¯ç¶²", "VR/AR", "5G", "æ©Ÿå™¨å­¸ç¿’", "é›²ç«¯"],
    "è‡ªç„¶": ["å…‰åˆä½œç”¨", "ç”Ÿæ…‹ç³»", "æ¼”åŒ–", "å€™é³¥é·å¾™", "çŠç‘šç¤", "ç†±å¸¶é›¨æ—", "æ¥µåœ°"],
    "è—è¡“": ["å°è±¡æ´¾", "æ¥µç°¡ä¸»ç¾©", "å·´æ´›å…‹", "ç«‹é«”æ´¾", "è¡—é ­è—è¡“", "äº’å‹•è—è¡“"],
    "å¿ƒç†": ["å¿ƒæµ", "èªçŸ¥åèª¤", "æ­£å¿µ", "æ½›æ„è­˜", "åŒç†å¿ƒ", "å‰µå‚·å¾Œæˆé•·"],
    "å•†æ¥­": ["ç²¾å¯¦å‰µæ¥­", "è—æµ·ç­–ç•¥", "è¨‚é–±ç¶“æ¿Ÿ", "å…±äº«ç¶“æ¿Ÿ", "å¹³å°ç¶“æ¿Ÿ"],
    "å“²å­¸": ["å­˜åœ¨ä¸»ç¾©", "åŠŸåˆ©ä¸»ç¾©", "æ–¯å¤šè‘›", "ç¦ªå®—", "è˜‡æ ¼æ‹‰åº•å¼"],
    "æ­·å²": ["æ–‡è—å¾©èˆˆ", "å·¥æ¥­é©å‘½", "è³‡è¨Šæ™‚ä»£", "å¤§èˆªæµ·æ™‚ä»£", "å•Ÿè’™é‹å‹•"],
    "ç”Ÿç‰©": ["ä»¿ç”Ÿå­¸", "ç¾¤é«”æ™ºæ…§", "é©è€…ç”Ÿå­˜", "å…±ç”Ÿé—œä¿‚", "åŸºå› çªè®Š"],
}


class FragmentCollector:
    """ç¢ç‰‡æ”¶é›†å™¨åŸºåº•é¡åˆ¥"""

    async def collect(self, topic: str, count: int, randomness: float = 0.5) -> list[Fragment]:
        raise NotImplementedError


class QuotesCollector(FragmentCollector):
    """åè¨€é‡‘å¥æ”¶é›†å™¨"""

    async def collect(self, topic: str, count: int, randomness: float = 0.5) -> list[Fragment]:
        selected = random.sample(CREATIVITY_QUOTES, min(count, len(CREATIVITY_QUOTES)))
        return [Fragment(content=q, source=FragmentSource.QUOTES, relevance=0.3) for q in selected]


class RandomConceptCollector(FragmentCollector):
    """éš¨æ©Ÿæ¦‚å¿µæ”¶é›†å™¨"""

    async def collect(self, topic: str, count: int, randomness: float = 0.5) -> list[Fragment]:
        # æ··åˆéš¨æ©Ÿæ¦‚å¿µå’Œè·¨é ˜åŸŸæ¦‚å¿µ
        fragments = []

        # éš¨æ©Ÿæ¦‚å¿µ
        n_random = int(count * 0.6)
        random_items = random.sample(RANDOM_CONCEPTS, min(n_random, len(RANDOM_CONCEPTS)))
        fragments.extend(
            [
                Fragment(content=f"éš¨æ©Ÿæ¦‚å¿µï¼š{c}", source=FragmentSource.RANDOM, relevance=0.2)
                for c in random_items
            ]
        )

        # è·¨é ˜åŸŸæ¦‚å¿µ
        n_cross = count - n_random
        all_cross = []
        for domain, concepts in CROSS_DOMAIN_CONCEPTS.items():
            for c in concepts:
                all_cross.append(f"{domain}é ˜åŸŸï¼š{c}")

        cross_items = random.sample(all_cross, min(n_cross, len(all_cross)))
        fragments.extend(
            [Fragment(content=c, source=FragmentSource.RANDOM, relevance=0.3) for c in cross_items]
        )

        return fragments


class DuckDuckGoCollector(FragmentCollector):
    """DuckDuckGo æœå°‹æ”¶é›†å™¨"""

    async def collect(self, topic: str, count: int, randomness: float = 0.5) -> list[Fragment]:
        try:
            from duckduckgo_search import DDGS

            fragments = []

            # ä¸»é¡Œç›¸é—œæœå°‹
            async with DDGS() as ddgs:
                results = list(ddgs.text(f"{topic} å‰µæ–°", max_results=count // 2))
                for r in results:
                    fragments.append(
                        Fragment(
                            content=f"ğŸ” {r.get('title', '')}: {r.get('body', '')[:100]}...",
                            source=FragmentSource.DUCKDUCKGO,
                            relevance=0.7,
                        )
                    )

                # éš¨æ©Ÿå»¶ä¼¸æœå°‹
                if randomness > 0.3:
                    random_word = random.choice(RANDOM_CONCEPTS)
                    random_results = list(ddgs.text(f"{random_word} è¶¨å‹¢", max_results=count // 2))
                    for r in random_results:
                        fragments.append(
                            Fragment(
                                content=f"ğŸ² {r.get('title', '')}: {r.get('body', '')[:100]}...",
                                source=FragmentSource.DUCKDUCKGO,
                                relevance=0.3,
                            )
                        )

            return fragments[:count]

        except Exception as e:
            logger.warning(f"DuckDuckGo æœå°‹å¤±æ•—: {e}")
            return []


class SoupAssembler:
    """å‰µæ„æ¹¯çµ„è£å™¨"""

    def __init__(self):
        self.collectors: dict[str, FragmentCollector] = {
            "quotes": QuotesCollector(),
            "random": RandomConceptCollector(),
            "duckduckgo": DuckDuckGoCollector(),
        }

    async def collect_fragments(
        self,
        topic: str,
        sources: list[str],
        count_per_source: int = 5,
        randomness: float = 0.5,
    ) -> list[Fragment]:
        """å¾å¤šå€‹ä¾†æºæ”¶é›†ç¢ç‰‡"""
        all_fragments = []

        for source in sources:
            collector = self.collectors.get(source)
            if collector:
                try:
                    fragments = await collector.collect(
                        topic=topic,
                        count=count_per_source,
                        randomness=randomness,
                    )
                    all_fragments.extend(fragments)
                except Exception as e:
                    logger.warning(f"æ”¶é›† {source} å¤±æ•—: {e}")

        return all_fragments

    def assemble_soup(
        self,
        topic: str,
        fragments: list[Fragment],
        topic_repetition: int = 5,
        trigger_categories: list[str] | None = None,
    ) -> SparkSoupResult:
        """çµ„è£å‰µæ„æ¹¯"""

        # ç¢ºä¿å¤šæ¨£æ€§ï¼šäº¤éŒ¯æ’åˆ—ä¸åŒä¾†æºçš„ç¢ç‰‡
        fragments = self._ensure_diversity(fragments)

        # é¸æ“‡è§¸ç™¼è©
        trigger_words = self._select_triggers(
            trigger_categories or ["combination", "inversion", "perspective"]
        )

        # çµ„è£
        soup_parts = []
        interval = max(1, len(fragments) // (topic_repetition + 1))
        trigger_index = 0

        # é–‹é ­ï¼šä¸»é¡Œå®£å‘Š
        soup_parts.append(f"ğŸ¯ **ä¸»è¦ä¸»é¡Œ**: {topic}")
        soup_parts.append("---")
        soup_parts.append("ä»¥ä¸‹æ˜¯å¤šå…ƒç¢ç‰‡è³‡è¨Šï¼Œè«‹å¾ä¸­å°‹æ‰¾æ„å¤–é€£çµï¼š\n")

        for i, fragment in enumerate(fragments):
            # æ¯éš” N å€‹ç¢ç‰‡æ’å…¥ä¸»é¡ŒéŒ¨å®š
            if i > 0 and i % interval == 0:
                soup_parts.append(f"\nğŸ¯ **æé†’ä¸»é¡Œ**: {topic}\n")

                # æ¯æ¬¡éŒ¨å®šæ™‚ä¹ŸåŠ å…¥ä¸€å€‹è§¸ç™¼è©
                if trigger_index < len(trigger_words):
                    soup_parts.append(f"ğŸ’¡ **ç™¼æƒ³æç¤º**: {trigger_words[trigger_index]}")
                    trigger_index += 1

            soup_parts.append(str(fragment))

        # çµå°¾ï¼šå†æ¬¡å¼·èª¿ä¸»é¡Œ
        soup_parts.append("\n---")
        soup_parts.append(f"ğŸ¯ **æ ¸å¿ƒä¸»é¡Œ**: {topic}")
        soup_parts.append("\nè«‹åŸºæ–¼ä»¥ä¸Šç¢ç‰‡ï¼Œç‚ºé€™å€‹ä¸»é¡Œç”¢ç”Ÿå‰µæ„æƒ³æ³•ã€‚å°‹æ‰¾æ„å¤–çš„é€£çµï¼")

        soup = "\n".join(soup_parts)

        # è¨ˆç®—å¤šæ¨£æ€§åˆ†æ•¸
        diversity_score = self._calculate_diversity(fragments)

        return SparkSoupResult(
            soup=soup,
            topic=topic,
            fragments_used=fragments,
            diversity_score=diversity_score,
            trigger_words_used=trigger_words[:trigger_index],
        )

    def _ensure_diversity(self, fragments: list[Fragment]) -> list[Fragment]:
        """ç¢ºä¿ç¢ç‰‡å¤šæ¨£æ€§ï¼ˆäº¤éŒ¯æ’åˆ—ï¼‰"""
        from collections import defaultdict

        by_source = defaultdict(list)
        for f in fragments:
            by_source[f.source].append(f)

        result = []
        while any(by_source.values()):
            for source in list(by_source.keys()):
                if by_source[source]:
                    result.append(by_source[source].pop(0))

        return result

    def _select_triggers(self, categories: list[str], count: int = 5) -> list[str]:
        """é¸æ“‡è§¸ç™¼è©"""
        selected = []
        for cat in categories:
            if cat in TRIGGER_WORDS:
                selected.extend(random.sample(TRIGGER_WORDS[cat], min(2, len(TRIGGER_WORDS[cat]))))
        return selected[:count]

    def _calculate_diversity(self, fragments: list[Fragment]) -> float:
        """è¨ˆç®—å¤šæ¨£æ€§åˆ†æ•¸"""
        if not fragments:
            return 0.0

        # ä¾†æºå¤šæ¨£æ€§
        sources = set(f.source for f in fragments)
        source_diversity = len(sources) / len(FragmentSource)

        # ç›¸é—œæ€§åˆ†å¸ƒï¼ˆè¶Šåˆ†æ•£è¶Šå¥½ï¼‰
        relevances = [f.relevance for f in fragments]
        if len(relevances) > 1:
            import statistics

            try:
                relevance_spread = statistics.stdev(relevances)
            except statistics.StatisticsError:
                relevance_spread = 0
        else:
            relevance_spread = 0

        return min(1.0, (source_diversity * 0.6 + relevance_spread * 0.4 + 0.3))


# === ä¸»è¦å…¥å£ ===

_assembler: SoupAssembler | None = None


def get_assembler() -> SoupAssembler:
    """å–å¾— SoupAssembler å–®ä¾‹"""
    global _assembler
    if _assembler is None:
        _assembler = SoupAssembler()
    return _assembler


async def spark_soup(
    topic: str,
    fragment_count: int = 20,
    topic_repetition: int = 5,
    auto_search: bool = True,
    custom_fragments: list[str] | None = None,
    trigger_categories: list[str] | None = None,
    randomness: float = 0.5,
) -> SparkSoupResult:
    """
    çµ„è£ã€Œå‰µæ„æ¹¯ã€- ç”¨ç¢ç‰‡åŒ–è³‡è¨Šå¡«å…… context

    Args:
        topic: ä¸»é¡Œï¼ˆæœƒåœ¨ soup ä¸­é‡è¤‡å¤šæ¬¡é¿å…éºå¿˜ï¼‰
        fragment_count: ç¢ç‰‡æ•¸é‡ï¼ˆé è¨­ 20ï¼‰
        topic_repetition: ä¸»é¡Œé‡è¤‡æ¬¡æ•¸ï¼ˆé è¨­ 5ï¼‰
        auto_search: æ˜¯å¦è‡ªå‹•æœå°‹å¤–éƒ¨è³‡è¨Š
        custom_fragments: ä½¿ç”¨è€…è‡ªè¨‚ç¢ç‰‡
        trigger_categories: è§¸ç™¼è©é¡åˆ¥ ["combination", "inversion", "scale", "time", "perspective", "emotion"]
        randomness: éš¨æ©Ÿæ€§ 0-1ï¼ˆè¶Šé«˜è¶Šéš¨æ©Ÿï¼‰

    Returns:
        SparkSoupResult
    """
    assembler = get_assembler()

    # æ±ºå®šä¾†æº
    sources = ["quotes", "random"]
    if auto_search:
        sources.append("duckduckgo")

    # è¨ˆç®—å„ä¾†æºæ•¸é‡
    count_per_source = max(3, fragment_count // len(sources))

    # æ”¶é›†ç¢ç‰‡
    fragments = await assembler.collect_fragments(
        topic=topic,
        sources=sources,
        count_per_source=count_per_source,
        randomness=randomness,
    )

    # åŠ å…¥ä½¿ç”¨è€…è‡ªè¨‚ç¢ç‰‡
    if custom_fragments:
        for cf in custom_fragments:
            fragments.append(
                Fragment(
                    content=cf,
                    source=FragmentSource.USER,
                    relevance=0.8,
                )
            )

    # éš¨æ©Ÿæ‰“äº‚ï¼ˆä½†ä¿æŒä¸€å®šçµæ§‹ï¼‰
    random.shuffle(fragments)

    # çµ„è£
    return assembler.assemble_soup(
        topic=topic,
        fragments=fragments[:fragment_count],
        topic_repetition=topic_repetition,
        trigger_categories=trigger_categories,
    )


async def collect_fragments(
    topic: str,
    sources: list[str] | None = None,
    count_per_source: int = 5,
    randomness: float = 0.5,
) -> list[Fragment]:
    """
    å¾å¤šå€‹ä¾†æºæ”¶é›†ç¢ç‰‡åŒ–è³‡è¨Š

    Args:
        topic: ç›¸é—œä¸»é¡Œ
        sources: è³‡æ–™ä¾†æº ["quotes", "random", "duckduckgo"]
        count_per_source: æ¯å€‹ä¾†æºæ”¶é›†æ•¸é‡
        randomness: éš¨æ©Ÿæ€§ 0-1

    Returns:
        list[Fragment]
    """
    assembler = get_assembler()
    return await assembler.collect_fragments(
        topic=topic,
        sources=sources or ["quotes", "random"],
        count_per_source=count_per_source,
        randomness=randomness,
    )
